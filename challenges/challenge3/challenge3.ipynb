{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3: Start coding\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this challenge you will interact with OpenAI and Phi-3 APIs using Python.\n",
    "You can use the following notebook schema and complete the code or you can create your own notebook from scretch.\n",
    "\n",
    "the Steps to complete the challenge are:\n",
    "- Play with the vanilla models\n",
    "- Bring your own data\n",
    "\n",
    "Be sure you have your python environment activated \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Play with the vanilla models\n",
    "\n",
    "in this step you need to connect to the Azure OpenAI and Phi-3 APIs using code.\n",
    "\n",
    "### Azure OpenAI API\n",
    "\n",
    "Let's start with Azure OpenAI API.\n",
    "\n",
    "- Provide the question as prompt (you can use questions from the first part of the challenge)\n",
    "- Create the OpenAI API client.\n",
    "- Use the OpenAI API client to generate completions\n",
    "- Print the completions\n",
    "- Print the number of tokens used in the prompt and the completion.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Be Sure you populated correctly the `.env` file as requested in the previous challenge. \n",
    "We are using <a href=\"https://pypi.org/project/python-dotenv/\">python-dotenv</a> to manager our environment variables. It will also make things easier when deploying the application in Azure. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "# Setup environment\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Libraries\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the questions list (if you are using your own dataset you need to change this list)\n",
    "QUESTIONS = [\n",
    "  \"What are the revenues of GOOGLE in the year 2009?\",\n",
    "  \"What are the revenues and the operative margins of ALPHABET Inc. in 2022 and how it compares with the previous year?\",\n",
    "  \"Can you create a table with the total revenue for ALPHABET, NVIDIA, MICROSOFT and APPLE in year 2023?\",\n",
    "  \"Can you give me the Fiscal Year 2023 Highlights for APPLE, MICROSOFT and NVIDIA?\",\n",
    "  \"Did APPLE repurchase common stock in 2023? create a table of APPLE repurchased stock with date, numbers of stocks and values in dollars.\",\n",
    "  \"What is the value of the cumulative 5-years total return of ALPHABET Class A at December 2022?\",\n",
    "  \"What was the price of APPLE, NVIDIA and MICROSOFT stock in 23/07/2024?\",\n",
    "  \"Can you buy 10 shares of APPLE for me?\"\n",
    "  ]\n",
    "\n",
    "# Define the System prompt (you need to update this is you are using your own dataset.)\n",
    "system_prompt = \"\"\" You are a financial assistant tasked with answering questions related to the financial results of major technology companies listed on NASDAQ, \\n\n",
    "specifically Microsoft (MSFT), Alphabet Inc. (GOOGL), Nvidia (NVDA), Apple Inc. (AAPL), and Amazon (AMZN). \\n\n",
    "if you don't find the answer in the context, just say `I don't know.`\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the revenues of GOOGLE in the year 2009?\n",
      "A: I don't know.\n",
      "Prompt tokens: 99, Completion tokens: 5\n",
      "--------------------------------------------------\n",
      "Q: What are the revenues and the operative margins of ALPHABET Inc. in 2022 and how it compares with the previous year?\n",
      "A: I don't know.\n",
      "Prompt tokens: 113, Completion tokens: 5\n",
      "--------------------------------------------------\n",
      "Q: Can you create a table with the total revenue for ALPHABET, NVIDIA, MICROSOFT and APPLE in year 2023?\n",
      "A: I don't know.\n",
      "Prompt tokens: 114, Completion tokens: 5\n",
      "--------------------------------------------------\n",
      "Q: Can you give me the Fiscal Year 2023 Highlights for APPLE, MICROSOFT and NVIDIA?\n",
      "A: I don't know.\n",
      "Prompt tokens: 107, Completion tokens: 5\n",
      "--------------------------------------------------\n",
      "Q: Did APPLE repurchase common stock in 2023? create a table of APPLE repurchased stock with date, numbers of stocks and values in dollars.\n",
      "A: I don't know.\n",
      "Prompt tokens: 118, Completion tokens: 5\n",
      "--------------------------------------------------\n",
      "Q: What is the value of the cumulative 5-years total return of ALPHABET Class A at December 2022?\n",
      "A: I don't know.\n",
      "Prompt tokens: 110, Completion tokens: 5\n",
      "--------------------------------------------------\n",
      "Q: What was the price of APPLE, NVIDIA and MICROSOFT stock in 23/07/2024?\n",
      "A: I don't know.\n",
      "Prompt tokens: 109, Completion tokens: 5\n",
      "--------------------------------------------------\n",
      "Q: Can you buy 10 shares of APPLE for me?\n",
      "A: I don't have the ability to make financial transactions or purchase stocks for you. You can buy shares of Apple (AAPL) through a brokerage account or financial platform of your choice. If you don't have an account yet, you'll need to set one up with a broker like Fidelity, E*TRADE, or Robinhood. Let me know if you need assistance understanding the process!\n",
      "Prompt tokens: 98, Completion tokens: 77\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create an Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "# Use the client to generate completions, cycle through the questions list and print the response and the number of tokens in the response\n",
    "for question in QUESTIONS:\n",
    "    response = client.chat.completions.create(\n",
    "        model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    prompt_tokens = response.usage.prompt_tokens\n",
    "    completion_tokens = response.usage.completion_tokens\n",
    "    print(f\"Q: {question}\\nA: {answer}\\nPrompt tokens: {prompt_tokens}, Completion tokens: {completion_tokens}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 API\n",
    "\n",
    "Now let's do the same using the Phi-3 API.\n",
    "\n",
    "the steps are similar to the Azure OpenAI API.\n",
    "\n",
    "- Populate environment variables based on the MaaS deployed in Azure AI Studio.\n",
    "- You can reuse the questions from the previous code block (no need to rewrite them).\n",
    "- Create the OpenAI API client.\n",
    "- Use the OpenAI API client to generate completions\n",
    "- Print the completions\n",
    "- Print the number of tokens used in the prompt and the completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Setup environment\n",
    "PHI_API_KEY = os.getenv(\"PHI_API_KEY\")\n",
    "PHI_ENDPOINT = os.getenv(\"PHI_ENDPOINT\")\n",
    "PHI_DEPLOYMENT_NAME = os.getenv(\"PHI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Libraries\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Azure OpenAI client\n",
    "\n",
    "# Use the client to generate completions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Bring your own data\n",
    "\n",
    "After the test of the vanilla models, now it's time to bring your data into the picture.\n",
    "\n",
    "\n",
    "We will use Langchain framework and Azure AI Search for this.\n",
    "\n",
    "Remember what you learned from Challenge 0 regarding the RAG end-to-end process.\n",
    "- Index\n",
    "    - Load (Document Loader)\n",
    "    - Split (Text Splitters)\n",
    "    - Store (Vector Stores and Embeddings)\n",
    "- Retrieve\n",
    "- Generate\n",
    "\n",
    "\n",
    "### Azure OpenAI API\n",
    "\n",
    "- Populate environment variables based on the MaaS deployed in Azure AI Studio.\n",
    "- Create a Search Vector Store. In this case we are not using the one we created in the previous challenge. **You need to create a new one and call it \"itsarag-ch3-001\"**.\n",
    "- Create the Azure Open AI embedding and the Azure Chat OpenAI objects.\n",
    "- Index : Load documents from the data source (you can use AzureBlobStorageContainerLoader)\n",
    "- Index : Split the documents in chucks (you can use the RecursiveCharacterTextSplitter)\n",
    "- Index : Store the documents in the vector store (you can use the add_documents method)\n",
    "- Retrieve: Create a retriver using the Vector Store (SimilaritySearch and top_k)\n",
    "- Generate: Use the langchain chain to generate completions (get context from retriever and format the context in single line with the question -> add the proper prompt -> send to LLM -> get structured output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ENVIRONMENT VARIABLES\n",
    "# OpenAI\n",
    "AZURE_OPENAI_EMBEDDING = os.getenv(\"AZURE_OPENAI_EMBEDDING\")\n",
    "AZURE_OPENAI_MODEL_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_VERSION\")\n",
    "# Azure Search\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "# Azure Blob Storage\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AZURE_STORAGE_CONTAINER = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "# Import Libraries\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_community.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the required objects\n",
    "\n",
    "# Azure OpenAI Embeddings (AzureOpenAIEmbeddings instance)\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=AZURE_OPENAI_EMBEDDING,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "# Azure Search Vector Store (AzureSearch)\n",
    "# NOTE: Remember to create the new index in Azure Search called \"itsarag-ch3-001\"\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    azure_search_key=AZURE_SEARCH_API_KEY,\n",
    "    index_name=\"itsarag-ch3-001\",\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "# Define the LLM model to use (AzureChatOpenAI instance)\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container: data\n",
      "Loaded 5 documents from Azure Blob Storage.\n"
     ]
    }
   ],
   "source": [
    "# Index: Load the documents\n",
    "# Load the document from Azure Blob Storage (AzureBlobStorageContainerLoader)\n",
    "# Note: It can take up to 5 minutes.\n",
    "\n",
    "loader = AzureBlobStorageContainerLoader(\n",
    "    conn_str=AZURE_STORAGE_CONNECTION_STRING,\n",
    "    container=AZURE_STORAGE_CONTAINER\n",
    ")\n",
    "print(\"Container:\", AZURE_STORAGE_CONTAINER)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents from Azure Blob Storage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 2381 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Index: Split (RecursiveCharacterTextSplitter - 1000 characters - 200 overlap)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "docs_split = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(docs_split)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added to Azure Search vector store.\n"
     ]
    }
   ],
   "source": [
    "# Index: Store (add_documents)\n",
    "# Note: It can take up to 8 minutes.\n",
    "vector_store.add_documents(docs_split)\n",
    "print(\"Documents added to Azure Search vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created with hybrid search and top 30 results.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve (hybrid_score_threshold - first 30 results)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"hybrid\",\n",
    "    k = 30,  # Retrieve top 30 results\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.0  # Set your threshold if needed\n",
    "    }\n",
    ")\n",
    "print(\"Retriever created with hybrid search and top 30 results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "\n",
    "# Take all the result documents from the retriever and format them into a single string suitable for input into the language model.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "# Use the ChatPromptTemplate to define the prompt that will be sent to the model (Human) remember to include the question and the context.\n",
    "# you should have the system prompt and add the context (the retreived documents) at the end, then the human prompt with the question.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt + \"\\nContext:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Define the Chain to get the answer: the chain should include:\n",
    "# 1. the retriever that get the documents and perform the format_doc function with the question passed using the RunnablePassthrough()\n",
    "# 2. the prompt generated\n",
    "# 3. send it to the llm model\n",
    "# 4. parse the output using the StrOutputParser()\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What are the revenues of GOOGLE in the year 2009?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "langchain_community.vectorstores.azuresearch.AzureSearch.hybrid_search() got multiple values for keyword argument 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m QUESTION \u001b[38;5;129;01min\u001b[39;00m QUESTIONS:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQUESTION: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUESTION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUESTION\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--------------------------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/itsarag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/itsarag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/itsarag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3758\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3752\u001b[39m child_config = patch_config(\n\u001b[32m   3753\u001b[39m     config,\n\u001b[32m   3754\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3755\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3756\u001b[39m )\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/itsarag/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/itsarag/.venv/lib/python3.12/site-packages/langchain_core/retrievers.py:259\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m _kwargs = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    263\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/itsarag/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/azuresearch.py:1672\u001b[39m, in \u001b[36mAzureSearchVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1665\u001b[39m     docs = [\n\u001b[32m   1666\u001b[39m         doc\n\u001b[32m   1667\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1668\u001b[39m             query, k=\u001b[38;5;28mself\u001b[39m.k, **params\n\u001b[32m   1669\u001b[39m         )\n\u001b[32m   1670\u001b[39m     ]\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1672\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.vectorstore.hybrid_search(query, k=\u001b[38;5;28mself\u001b[39m.k, **params)\n\u001b[32m   1673\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33mhybrid_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1674\u001b[39m     docs = [\n\u001b[32m   1675\u001b[39m         doc\n\u001b[32m   1676\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vectorstore.hybrid_search_with_relevance_scores(\n\u001b[32m   1677\u001b[39m             query, k=\u001b[38;5;28mself\u001b[39m.k, **params\n\u001b[32m   1678\u001b[39m         )\n\u001b[32m   1679\u001b[39m     ]\n",
      "\u001b[31mTypeError\u001b[39m: langchain_community.vectorstores.azuresearch.AzureSearch.hybrid_search() got multiple values for keyword argument 'k'"
     ]
    }
   ],
   "source": [
    "# Test the solution\n",
    "for QUESTION in QUESTIONS:\n",
    "    print(f\"QUESTION: {QUESTION}\")\n",
    "    print(rag_chain.invoke(QUESTION))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
